{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils import shuffle\n",
    "import keras\n",
    "from tflearn.layers import conv_1d, embedding, max_pool_1d, fully_connected, flatten, regression, input_data, dropout\n",
    "import tflearn\n",
    "import tflearn\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirs = os.listdir('./aclImdb/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "labels = []\n",
    "for i in dirs:\n",
    "    if i == 'neg':\n",
    "        for j in os.listdir('./aclImdb/train/neg'):\n",
    "            with open('./aclImdb/train/neg/'+j, 'rb') as f:\n",
    "                text.append(f.read())\n",
    "                labels.append([1,0])\n",
    "    if i == 'pos':\n",
    "        for j in os.listdir('./aclImdb/train/pos'):\n",
    "            with open('./aclImdb/train/pos/'+j, 'rb') as f:\n",
    "                text.append(f.read())\n",
    "                labels.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_feature_map = 256\n",
    "nodes = 1024\n",
    "epochs = 10\n",
    "split = 2072\n",
    "\n",
    "vocab = set()\n",
    "for review in text[:1000]:\n",
    "    for word in str(review).split(' '):\n",
    "        vocab.add(word)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "def encode(text):\n",
    "    vector = []\n",
    "    for word in text.split(' '):\n",
    "        vector.append(word2index[word])\n",
    "    return vector\n",
    "\n",
    "reviews = []\n",
    "for review in text[:1000]:\n",
    "    reviews.append(encode(str(review)))\n",
    "reviews = pad_sequences(reviews, maxlen=100, value=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/khushmeetsingh/anaconda/lib/python3.6/site-packages/tflearn/initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From /Users/khushmeetsingh/anaconda/lib/python3.6/site-packages/tflearn/objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model = input_data(shape=[None, 100])\n",
    "model = embedding(model, input_dim=vocab_size, output_dim=128)\n",
    "conv1 = conv_1d(model, 50, filter_size=3, padding='same', activation='relu', regularizer='L2')\n",
    "conv2 = conv_1d(conv1, 50, filter_size=4, padding='same', activation='relu', regularizer='L2')\n",
    "pool = max_pool_1d(conv2, kernel_size=3)\n",
    "pool = dropout(pool, 0.5)\n",
    "pool = flatten(pool)\n",
    "pool = fully_connected(pool, 2, activation='softmax')\n",
    "reg = regression(pool, optimizer='adam', learning_rate=0.01, loss='categorical_crossentropy')\n",
    "mod = tflearn.DNN(reg, tensorboard_dir='./tflearn-grapgh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 1.499s\n",
      "| Adam | epoch: 005 | loss: 0.00000 - acc: 1.0000 -- iter: 0960/1000\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.00000\u001b[0m\u001b[0m | time: 1.598s\n",
      "| Adam | epoch: 005 | loss: 0.00000 - acc: 1.0000 -- iter: 1000/1000\n",
      "--\n",
      "Time taken -  8.940109729766846\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "mod.fit(reviews,np.array(labels[:1000]), n_epoch=5, shuffle=True, show_metric=True, batch_size=64)\n",
    "end = time.time()\n",
    "print('Time taken - ', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/Users/khushmeetsingh/Desktop/bitcoin-prediction/cnn.tfl is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "mod.save('cnn.tfl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
